{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e43ce9-c243-4046-8695-9c10f75f6a91",
   "metadata": {},
   "source": [
    "### Loading the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc525e8-3db2-4182-93b0-c401863af600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('azure.env',override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d3650-546a-4b4f-99b1-7151fd429e3f",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6e2f82-97c8-48f6-98a8-abe25c0e30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings,AzureChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain import hub\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import PostgresChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature\n",
    "from doc_intelligence import AzureAIDocumentIntelligenceLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd5389-4c40-463a-893a-c331525e7e33",
   "metadata": {},
   "source": [
    "### Uploading the document through Azure Document Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9771883b-0548-4db9-86b6-2ae107dce3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures:\n",
      "Figure #0 has the following spans: [{'offset': 102, 'length': 81}]\n",
      "Span #0: {'offset': 102, 'length': 81}\n",
      "Original figure content in markdown: \n",
      "<figure>\n",
      "\n",
      "![](figures/0)\n",
      "\n",
      "<!-- FigureContent=\"Azure Databricks\" -->\n",
      "\n",
      "</figure>\n",
      "\n",
      "\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 1, 'polygon': [0.9636, 0.9767, 4.2248, 0.9765, 4.2246, 1.4254, 0.9637, 1.4254]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.9636, 0.9767, 4.2246, 1.4254)\n",
      "\tFigure 0 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_0.png\n",
      "\tDescription of figure 0: The image features a simple design with a dark background. On the left, there is a stylized logo composed of three stacked shapes that resemble blocks or data cubes. To the right of the logo, the text \"Azure Databricks\" is displayed in a bold, modern font, indicating the branding of Azure's data analytics platform. The overall look is clean and professional, reflecting a technology-oriented theme.\n",
      "Figure #1 has the following spans: [{'offset': 1852, 'length': 80}]\n",
      "Span #0: {'offset': 1852, 'length': 80}\n",
      "Original figure content in markdown: cted: :unselected: :unselected: :unselected: :unselected: :unselected: :unselect\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 1, 'polygon': [0.9551, 9.589, 3.489, 9.5888, 3.4885, 10.0408, 0.9547, 10.0408]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.9551, 9.589, 3.4885, 10.0408)\n",
      "\tFigure 1 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_1.png\n",
      "\tDescription of figure 1: The image features the logo of Microsoft Azure, which is a cloud computing service. The logo consists of a collection of colored squares—red, green, blue, and yellow—arranged in a grid pattern, with the blue square overlaying a yellow square at the bottom left. To the right of the colored squares, the text \"Microsoft Azure\" is displayed in bold, white letters against a black background. The overall design is simple and modern, reflecting the technological focus of the brand.\n",
      "Figure #2 has the following spans: [{'offset': 5875, 'length': 38}]\n",
      "Span #0: {'offset': 5875, 'length': 38}\n",
      "Original figure content in markdown: - PageNumber=\"4\" -->\n",
      " :selected: :sele\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 4, 'polygon': [2.3276, 3.7236, 2.876, 3.7237, 2.876, 4.2559, 2.3276, 4.2558]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (2.3276, 3.7236, 2.876, 4.2559)\n",
      "\tFigure 2 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_2.png\n",
      "\tDescription of figure 2: The image features rows of circles in different shades of blue arranged against a black background. Each row consists of several blue and lighter blue circles. At the bottom of the image, there is a stylized hourglass shape in blue, which contains a gradient of blue tones. The overall design is simple and geometric, with a modern aesthetic.\n",
      "Figure #3 has the following spans: [{'offset': 6568, 'length': 38}]\n",
      "Span #0: {'offset': 6568, 'length': 38}\n",
      "Original figure content in markdown: ome of the challenges of harnessing st\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 4, 'polygon': [2.3444, 6.7994, 2.8778, 6.7995, 2.878, 7.3419, 2.3447, 7.3418]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (2.3444, 6.7994, 2.878, 7.3419)\n",
      "\tFigure 3 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_3.png\n",
      "\tDescription of figure 3: The image features a central black square with light blue curly braces symbols inside it, indicating code or programming. Surrounding this central square are several circles in black and varying shades of blue, connected by straight blue lines. This design suggests a network or system, likely symbolizing connections in technology, coding, or data communication. The overall aesthetic conveys a modern and digital theme.\n",
      "Figure #4 has the following spans: [{'offset': 8724, 'length': 75}]\n",
      "Span #0: {'offset': 8724, 'length': 75}\n",
      "Original figure content in markdown: om many different parts of the organization. These data are then normalized\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 6, 'polygon': [0.9553, 2.6681, 2.0603, 2.6685, 2.0593, 3.5311, 0.9547, 3.5305]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.9553, 2.6681, 2.0593, 3.5311)\n",
      "\tFigure 4 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_4.png\n",
      "\tDescription of figure 4: The image features a logo for \"Delta Lake.\" It includes a stylized triangle in a bright turquoise color, which appears to represent a mountain or water wave, positioned above the text. The text \"DELTA LAKE\" is displayed below in bold, dark gray letters. The design conveys a sense of nature and tranquility, suggesting a connection to lakes or outdoor activities.\n",
      "Figure #5 has the following spans: [{'offset': 9264, 'length': 184}]\n",
      "Span #0: {'offset': 9264, 'length': 184}\n",
      "Original figure content in markdown: lures may cause data between stages to either drop on the floor or be duplicated.\n",
      "\n",
      "· Partitioning alone does not scale for multidimensional data.\n",
      "\n",
      ". Standard tables do not allow the co\n",
      "\tCaption: Databricks Delta Table\n",
      "\tCaption bounding region: [{'pageNumber': 6, 'polygon': [4.1301, 4.717, 5.8306, 4.7122, 5.8311, 4.8793, 4.1306, 4.8841]}]\n",
      "\tFigure body bounding regions: {'pageNumber': 6, 'polygon': [2.5501, 4.9623, 7.3228, 4.9592, 7.3252, 8.3921, 2.5533, 8.394]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (2.5501, 4.9623, 7.3252, 8.3921)\n",
      "\tFigure 5 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_5.png\n",
      "\tDescription of figure 5: The image titled \"Databricks Delta Table\" consists of a structured graphical representation of components related to a Delta Table:\n",
      "\n",
      "1. **Delta Table**: At the top, there is an icon depicting a table (represented with rows and columns) labeled \"Delta Table.\"\n",
      "\n",
      "2. **Versioned Parquet Files**: Below the Delta Table, there are several icons representing versioned Parquet files, indicating data storage and management.\n",
      "\n",
      "3. **DBFS**: Centered in the lower section, there is a label for \"DBFS\" (Databricks File System), which is the storage layer.\n",
      "\n",
      "4. **Indexes & Stats**: To the left, there's an icon labeled \"Indexes & Stats,\" which suggests that indexing and statistical information are part of the system.\n",
      "\n",
      "5. **Delta Log**: To the right, there's an icon labeled \"Delta Log,\" which likely refers to the logs that track changes in the Delta Table.\n",
      "\n",
      "The layout visually organizes these components to illustrate the architecture of the Databricks Delta Table system.\n",
      "Figure #6 has the following spans: [{'offset': 14073, 'length': 117}]\n",
      "Span #0: {'offset': 14073, 'length': 117}\n",
      "Original figure content in markdown: :\n",
      "<!-- PageNumber=\"8\" -->\n",
      " :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :s\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 9, 'polygon': [2.5127, 6.8325, 7.3428, 6.8333, 7.3429, 8.7425, 2.513, 8.7412]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (2.5127, 6.8325, 7.3429, 8.7425)\n",
      "\tFigure 6 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_6.png\n",
      "\tDescription of figure 6: The image illustrates a data organization schema, specifically focusing on how data can be partitioned and optimized:\n",
      "\n",
      "1. **Partitioned on Date**: The diagram shows folders or containers organized by \"Year\" and \"Month.\" This indicates that data is arranged based on temporal criteria, making it easier to manage large datasets over time.\n",
      "\n",
      "2. **Optimized with City**: Below the partitioned structure, there’s a different approach where data is optimized by including \"City\" as a criterion. This likely suggests that within each partition (like Year/Month), data is further categorized based on city, facilitating faster access and retrieval.\n",
      "\n",
      "The visual elements are color-coded, with blue and black folders representing different organizational strategies, and arrows indicating directionality or flow. This helps visualize how data organization can impact efficiency.\n",
      "Figure #7 has the following spans: [{'offset': 17506, 'length': 314}]\n",
      "Span #0: {'offset': 17506, 'length': 314}\n",
      "Original figure content in markdown: ely suggests that within each partition (like Year/Month), data is further categorized based on city, facilitating faster access and retrieval.\n",
      "\n",
      "The visual elements are color-coded, with blue and black folders representing different organizational strategies, and arrows indicating directionality or flow. This hel\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 12, 'polygon': [2.5442, 2.2733, 7.3168, 2.2697, 7.3183, 5.2806, 2.5464, 5.2832]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (2.5442, 2.2733, 7.3183, 5.2806)\n",
      "\tFigure 7 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_7.png\n",
      "\tDescription of figure 7: The image illustrates the processes of a \"Successful Write\" and a \"Failed Write\" in a database context:\n",
      "\n",
      "1. **Successful Write** (top section):\n",
      "   - A write request is initiated for a table in state \\(T_n\\).\n",
      "   - The table \\(T_n\\) is copied as \\(T_{n+1}\\) for the write, while \\(T_n\\) remains available.\n",
      "   - The write completes successfully.\n",
      "   - Finally, \\(T_n\\) is replaced with \\(T_{n+1}\\).\n",
      "\n",
      "2. **Failed Write** (bottom section):\n",
      "   - Similar to the successful write, a write request starts with the table in state \\(T_n\\).\n",
      "   - The table \\(T_n\\) is again copied as \\(T_{n+1}\\) for the write, with \\(T_n\\) still available.\n",
      "   - However, the write fails.\n",
      "   - As a result, \\(T_n\\) continues to be used.\n",
      "\n",
      "Both sections highlight the importance of maintaining availability of the original table during write operations, whether successful or failed.\n",
      "Figure #8 has the following spans: [{'offset': 20169, 'length': 152}]\n",
      "Span #0: {'offset': 20169, 'length': 152}\n",
      "Original figure content in markdown: vitable partial or failed writes risk corrupting the data. Delta Lake employs an \"all or nothing\" ACID transaction approach to prevent such corruption.\n",
      "\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 13, 'polygon': [2.5063, 5.2602, 7.3253, 5.2609, 7.3207, 6.5693, 2.5025, 6.5684]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (2.5063, 5.2602, 7.3207, 6.5693)\n",
      "\tFigure 8 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_8.png\n",
      "\tDescription of figure 8: The image illustrates a data processing concept involving a \"Delta Table.\" \n",
      "\n",
      "- On the left, there are multiple input streams, represented by arrows pointing toward the delta table. \n",
      "- The central focus is the \"Delta Table,\" shown in a rectangular box and highlighted in blue. \n",
      "- To the right, there is a note indicating that the table is available for queries at all times.\n",
      "- Below the delta table, an arrow points upward labeled \"Batch updates,\" suggesting that updates to the table occur in batches.\n",
      "\n",
      "Overall, it represents a system where data streams feed into a table that can be queried continuously, with updates processed in batches.\n",
      "Figure #9 has the following spans: [{'offset': 22701, 'length': 75}]\n",
      "Span #0: {'offset': 22701, 'length': 75}\n",
      "Original figure content in markdown: ng must be addressed using new tables. Delta Lake on Azure Databricks provi\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 14, 'polygon': [0.9546, 2.6845, 2.06, 2.6849, 2.059, 3.5357, 0.9539, 3.5351]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.9546, 2.6845, 2.059, 3.5357)\n",
      "\tFigure 9 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_9.png\n",
      "\tDescription of figure 9: The image features a logo for \"Delta Lake.\" It includes a stylized triangle in a bright turquoise color positioned above a wavy blue shape, likely representing water. Beneath this graphic, the words \"DELTA LAKE\" are displayed in bold, dark gray letters, with \"DELTA\" and \"LAKE\" styled in a similar font emphasizing clarity and readability. The design suggests a connection to nature, particularly lakes or water bodies.\n",
      "Figure #10 has the following spans: [{'offset': 24185, 'length': 76}]\n",
      "Span #0: {'offset': 24185, 'length': 76}\n",
      "Original figure content in markdown:  a direct integration with Structured Streaming for low latency updates). It\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 16, 'polygon': [0.9551, 2.559, 2.0446, 2.5597, 2.0438, 3.4095, 0.9546, 3.4085]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.9551, 2.559, 2.0438, 3.4095)\n",
      "\tFigure 10 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_10.png\n",
      "\tDescription of figure 10: The image features a logo for \"Delta Lake.\" It consists of a stylized triangle shape in a bright aqua color, which suggests a body of water or a delta. Beneath the triangle, the words \"DELTA LAKE\" are displayed in bold, dark gray letters. The overall design conveys a clean and modern aesthetic, likely representing a water-related theme or service.\n",
      "Figure #11 has the following spans: [{'offset': 26414, 'length': 81}]\n",
      "Span #0: {'offset': 26414, 'length': 81}\n",
      "Original figure content in markdown: s to the table, so a read before and after an OPTIMIZE has the same results. It s\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 18, 'polygon': [0.9674, 9.5852, 3.7469, 9.5849, 3.7468, 10.0611, 0.9673, 10.0613]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.9674, 9.5852, 3.7468, 10.0611)\n",
      "\tFigure 11 cropped and saved as D:/MS/Git/AOAI_Samples/multimodal_rag/data/cropped\\Databricks Delta eBook_Final_cropped_image_11.png\n",
      "\tDescription of figure 11: The image features the logo of Microsoft Azure. The background is a bright blue, and to the left, there are four colored squares arranged in a two-by-two grid. The top-left square is orange, the top-right square is green, the bottom-left square is blue, and the bottom-right square is yellow. Next to the squares, the text \"Microsoft Azure\" is prominently displayed in large, bold white letters.\n"
     ]
    }
   ],
   "source": [
    "loader = AzureAIDocumentIntelligenceLoader(file_path=r'D:\\MS\\Learning\\Databrick\\Databricks Delta eBook_Final.pdf', \n",
    "                                           api_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\"), \n",
    "                                           api_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\"),\n",
    "                                           api_model=\"prebuilt-layout\",\n",
    "                                           api_version=\"2024-02-29-preview\",\n",
    "                                           mode='markdown',\n",
    "                                           analysis_features = [DocumentAnalysisFeature.OCR_HIGH_RESOLUTION])\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87844736-64d5-496e-8205-3408e20a7fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       ":unselected: :selected: :unselected: :unselected: :unselected:\n",
       "O\n",
       " :selected: :unselected: :unselected:\n",
       "<figure>\n",
       "\n",
       "![](figures/0)<!-- FigureContent=\"The image features a simple design with a dark background. On the left, there is a stylized logo composed of three stacked shapes that resemble blocks or data cubes. To the right of the logo, the text \"Azure Databricks\" is displayed in a bold, modern font, indicating the branding of Azure's data analytics platform. The overall look is clean and professional, reflecting a technology-oriented theme.\" --></figure>\n",
       "\n",
       "\n",
       "Delta Lake on Azure Databricks\n",
       "===\n",
       " :unselected: :selected:\n",
       "1\n",
       " :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected:\n",
       ".\n",
       " :selected: :selected: :selected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected: :selected:\n",
       ".\n",
       " :selected:\n",
       ".\n",
       " :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected: :selected: :selected: :unselected: :unselected: :unselected:\n",
       ".\n",
       " :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected: :selected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected: :selected: :selected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected: :selected:\n",
       ".\n",
       " :selected: :selected:\n",
       ".\n",
       " :unselected: :unselected:\n",
       "<figure>\n",
       "\n",
       "![](figures/1)<!-- FigureContent=\"The image features the logo of Microsoft Azure, which is a cloud computing service. The logo consists of a collection of colored squares—red, green, blue, and yellow—arranged in a grid pattern, with the blue square overlaying a yellow square at the bottom left. To the right of the colored squares, the text \"Microsoft Azure\" is displayed in bold, white letters against a black background. The overall design is simple and modern, reflecting the technological focus of the brand.\" --></figure>\n",
       "\n",
       " :unselected: :unselected: :unselected:\n",
       "<!-- PageNumber=\"2\" -->\n",
       "\n",
       "<!-- PageHeader=\"Delta Lake on Azure Databricks\" -->\n",
       "\n",
       "\n",
       "## Contents\n",
       "\n",
       "|||\n",
       "| - | - |\n",
       "| An \"under the hood\" look | 3 :unselected: :unselected: |\n",
       "| Challenges in Harnessing Data | 4 :selected: |\n",
       "| Delta Lake Architecture | 6 :unselected: |\n",
       "| Building and Maintaining Robust Pipelines | 8 :unselected: :unselected: |\n",
       "| Delta Lake on Azure Databricks Details | 8 :unselected: :unselected: |\n",
       "| Query performance. | 9 :unselected: |\n",
       "| Data indexing | 9 :unselected: |\n",
       "| Data skipping. | 9 :unselected: |\n",
       "| Compaction. | 10 :unselected: |\n",
       "| Data Caching | 10 :unselected: |\n",
       "| Data Reliability | 11 |\n",
       "| ACID Transactions | 11 |\n",
       "| Snapshot Isolation | 11 |\n",
       "| Schema Enforcement | 11 |\n",
       "| Exactly Once | 12 |\n",
       "| UPSERTS and DELETES Support. | 12 |\n",
       "| System Complexity ..... | 13 |\n",
       "| Delta Lake on Azure Databricks Best Practices | 14 |\n",
       "| Go Through Delta Lake | 14 |\n",
       "| Run OPTIMIZE Regularly | 14 |\n",
       "| Run VACUUM Regularly | 14 |\n",
       "| Batch Modifications | 15 |\n",
       "| Use DELETES | 15 |\n",
       "| Trying Delta Lake on Azure Databricks | 16 |\n",
       "| Resources | 17 |\n",
       "\n",
       "<!-- PageFooter=\"Copyright 2020 Microsoft Corporation\" -->\n",
       "\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"3\" -->\n",
       "\n",
       "An \"under the hood\" look\n",
       "===\n",
       "\n",
       "Delta Lake on Azure Databricks allows you to configure Delta Lake based on your workload patterns and has optimized layouts and indexes for fast interactive queries. Delta Lake is an open source storage layer that brings reliability to data lakes. It provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs.\n",
       "\n",
       "Designed for both batch and stream processing, Delta Lake also addresses concerns about system complexity. Its advanced architecture enables high reliability and low latency, using techniques such as schema validation, compaction, data skipping, and more to address pipeline development, data management and as well as query serving.\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"4\" -->\n",
       " :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "Challenges in Harnessing Data\n",
       "===\n",
       "\n",
       "To respond to ever-growing data volumes, many organizations are collecting their data in data lakes ahead of making it available for analysis. This has addressed some of the challenges of harnessing structured and unstructured data, but, unfortunately, data lakes suffer from some key challenges of their own:\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/2)<!-- FigureContent=\"The image features rows of circles in different shades of blue arranged against a black background. Each row consists of several blue and lighter blue circles. At the bottom of the image, there is a stylized hourglass shape in blue, which contains a gradient of blue tones. The overall design is simple and geometric, with a modern aesthetic.\" --></figure>\n",
       "\n",
       "\n",
       "\n",
       "## Query performance\n",
       "\n",
       "The ETL processes required by data lakes can add significant latency to the point that it may take hours before incoming data manifests in a query response. As a result, the users do not benefit from the latest data. Further, the longer query run times can be deemed unacceptable by users, especially when the scale increases.\n",
       "\n",
       "\n",
       "## Data reliability\n",
       "\n",
       "The complex data pipelines are error-prone and complicated, consuming resources significantly. Further, schema evolution as business needs change can be effort intensive. Finally, errors or gaps in incoming data, a common occurrence, can cause failures in downstream applications.\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/3)<!-- FigureContent=\"The image features a central black square with light blue curly braces symbols inside it, indicating code or programming. Surrounding this central square are several circles in black and varying shades of blue, connected by straight blue lines. This design suggests a network or system, likely symbolizing connections in technology, coding, or data communication. The overall aesthetic conveys a modern and digital theme.\" --></figure>\n",
       "\n",
       "\n",
       "\n",
       "## System complexity\n",
       "\n",
       "It is difficult to build flexible data engineering pipelines that combine streaming and batch analytics. These systems require complex and low-level code. Intervention during stream processing with batch correction or programming multiple streams from the same sources or to the same destinations is restricted.\n",
       "\n",
       "Practitioners typically organize their pipelines using a multi-hop architecture. The pipeline starts with a \"firehose\" of records from many different parts of the organization. These data are then normalized and enriched with dimension information.\n",
       "\n",
       "Following this, the data may be filtered down and aggregated for particular business objectives. Finally, high-level summaries of key business metrics might be created.\n",
       ":selected: :selected:\n",
       "<!-- PageNumber=\"5\" -->\n",
       "\n",
       "<!-- PageHeader=\"Challenges in Harnessing Data\" -->\n",
       "\n",
       "There are various challenges encountered during the pipeline stages:\n",
       "\n",
       "· Schema changes can break enrichment, joins, and transforms between stages.\n",
       "\n",
       ". Failures may cause data between stages to either drop on the floor or be duplicated.\n",
       "\n",
       "· Partitioning alone does not scale for multidimensional data.\n",
       "\n",
       ". Standard tables do not allow the combination of streaming and batch processes for optimal latencies.\n",
       "\n",
       "· Concurrent access suffers from inconsistent query results.\n",
       "\n",
       "· Failing streaming jobs can require resetting and restarting data processing.\n",
       "\n",
       "Delta Lake on Azure Databricks addresses the challenges faced by data engineering professionals in marshalling their data head-on. It offers a simple analytics architecture that can address both batch and stream use cases with high query performance and high data reliability.\n",
       "\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"6\" -->\n",
       "\n",
       "Delta Lake Architecture\n",
       "===\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/4)<!-- FigureContent=\"The image features a logo for \"Delta Lake.\" It includes a stylized triangle in a bright turquoise color, which appears to represent a mountain or water wave, positioned above the text. The text \"DELTA LAKE\" is displayed below in bold, dark gray letters. The design conveys a sense of nature and tranquility, suggesting a connection to lakes or outdoor activities.\" --></figure>\n",
       "\n",
       "\n",
       "The Delta Lake architecture provides an efficient and transactional way to work with large data sets stored as files on Azure Blob. It employs an ordered log (the Delta Lake transaction log) of atomic collections of actions (e.g. AddFile, RemoveFile, etc.). Delta Lake tables are build built atop the Azure Databricks File System (DBFS). which manifests:\n",
       "\n",
       "· Versioned Parquet files (based on Apache Parquet1)\n",
       "\n",
       "· Indexes and stats\n",
       "\n",
       "· The Delta Lake transaction log\n",
       "\n",
       "<figure>\n",
       "\n",
       "<figcaption>\n",
       "\n",
       "Databricks Delta Table\n",
       "\n",
       "</figcaption>\n",
       "\n",
       "![](figures/5)<!-- FigureContent=\"The image titled \"Databricks Delta Table\" consists of a structured graphical representation of components related to a Delta Table:\n",
       "\n",
       "1. **Delta Table**: At the top, there is an icon depicting a table (represented with rows and columns) labeled \"Delta Table.\"\n",
       "\n",
       "2. **Versioned Parquet Files**: Below the Delta Table, there are several icons representing versioned Parquet files, indicating data storage and management.\n",
       "\n",
       "3. **DBFS**: Centered in the lower section, there is a label for \"DBFS\" (Databricks File System), which is the storage layer.\n",
       "\n",
       "4. **Indexes & Stats**: To the left, there's an icon labeled \"Indexes & Stats,\" which suggests that indexing and statistical information are part of the system.\n",
       "\n",
       "5. **Delta Log**: To the right, there's an icon labeled \"Delta Log,\" which likely refers to the logs that track changes in the Delta Table.\n",
       "\n",
       "The layout visually organizes these components to illustrate the architecture of the Databricks Delta Table system.\" --></figure>\n",
       "\n",
       "\n",
       "Parquet is a columnar storage format and supports efficient compression and encoding schemes. In Delta Lake, the versioned Parquet files enable you to track the evolution of the data. Indexes and statistics about the files are maintained to increase query efficiency. The Delta Lake transaction log can be appended to by multiple writers that are mediated by optimistic\n",
       "\n",
       "1 https://parquet.apache.org/\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"7\" -->\n",
       "\n",
       "<!-- PageHeader=\"Delta Lake Architecture\" -->\n",
       "\n",
       "concurrency control that provide serializable ACID transactions. Changes to the table are stored as ordered atomic units called commits. The log can be read in parallel by a cluster of Spark executors.\n",
       "\n",
       "The Delta Lake on Azure Databricks design allows readers to efficiently query a snapshot of the state of a table, optionally filtering by partition value. The result is fast operations irrespective of the number of files.\n",
       "\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"8\" -->\n",
       " :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "Building and Maintaining Robust Pipelines\n",
       "===\n",
       "\n",
       "Designing and building robust pipelines is the first step in realizing value from one's data resources. The focus should be on:\n",
       "\n",
       "· Ingesting the data\n",
       "\n",
       "· Ensuring data quality upon ingest\n",
       "\n",
       "· Maintaining the data over time as enhancements and corrections need to be made\n",
       "\n",
       "· Managing the environment effectively as queries as being run.\n",
       "\n",
       "Delta Lake on Azure Databricks helps address challenges throughout the various pipeline stages and handles both batch and streaming data.\n",
       "\n",
       "\n",
       "## Delta Lake on Azure Databricks Details\n",
       "\n",
       "Delta Lake uses a number of techniques to address query performance, data reliability and system complexity.\n",
       ":selected: :selected:\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"9\" -->\n",
       " :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "Query performance\n",
       "===\n",
       "\n",
       "Query performance is a major driver of user satisfaction. The faster a query returns results, the sooner those results are available for using. With Delta Lake on Azure Databricks, query performance can be 10 to 100 times faster than with Apache Spark on Parquet. Delta Lake on Azure Databricks employs various techniques to deliver superior performance.\n",
       "\n",
       "\n",
       "## Data indexing\n",
       "\n",
       "Delta Lake creates and maintains indexes of the ingested data, which increases the querying speed significantly.\n",
       "\n",
       "\n",
       "## Data skipping\n",
       "\n",
       "Delta Lake maintains file statistics so that data subsets relevant to the query are used instead of entire tables-this partition pruning avoids processing data that is not relevant to the query. Multidimensional clustering (using Z-ordering algorithm) is used to enable this. This technique is particularly helpful in the case of complex queries.\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/6)<!-- FigureContent=\"The image illustrates a data organization schema, specifically focusing on how data can be partitioned and optimized:\n",
       "\n",
       "1. **Partitioned on Date**: The diagram shows folders or containers organized by \"Year\" and \"Month.\" This indicates that data is arranged based on temporal criteria, making it easier to manage large datasets over time.\n",
       "\n",
       "2. **Optimized with City**: Below the partitioned structure, there’s a different approach where data is optimized by including \"City\" as a criterion. This likely suggests that within each partition (like Year/Month), data is further categorized based on city, facilitating faster access and retrieval.\n",
       "\n",
       "The visual elements are color-coded, with blue and black folders representing different organizational strategies, and arrows indicating directionality or flow. This helps visualize how data organization can impact efficiency.\" --></figure>\n",
       "\n",
       "\n",
       "SELECT ... FROM flights\n",
       "\n",
       "WHERE date BETWEEN \"2020-Jun-01\" AND \"2020-Aug-31\" AND originating\\_city = \"Seattle\"\n",
       "\n",
       "Only read the relevant subsets (shown in blue) to fulfil query\n",
       ":selected: :selected:\n",
       "<!-- PageNumber=\"10\" -->\n",
       "\n",
       "<!-- PageHeader=\"Query Performance\" -->\n",
       "\n",
       "\n",
       "## Compaction\n",
       "\n",
       "Often, especially in the case of streaming data, a large number of small files are created as data is ingested. Storing and accessing these small files can be processing-intensive, slow and inefficient from a storage utilization perspective. Delta Lake on Azure Databricks manages file sizes (i.e., compacts or combines multiple small files into more efficient larger ones) to speed up query performance.\n",
       "\n",
       "\n",
       "## Data Caching\n",
       "\n",
       "Accessing data from storage repeatedly can slow query performance. Delta Lake automatically caches highly accessed data to speed access for queries.\n",
       "\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"11\" -->\n",
       " :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "Data Reliability\n",
       "===\n",
       "\n",
       "Reliable datasets are key to successful data analytics, whether it's feeding dashboards or enabling ML initiatives. Delta uses various techniques to achieve data reliability. By 'filtering' messy data and blocking access into Delta Lake, clean data sits in a Delta Lake on top of the data lake. Optimistic concurrency control between writes and snapshot isolation deliver ACID transactions for consistent reads during writes. Delta Lake also offers built-in data versioning for easy rollbacks and reproducing reports.\n",
       "\n",
       "\n",
       "## ACID Transactions\n",
       "\n",
       "The inevitable partial or failed writes risk corrupting the data. Delta Lake employs an \"all or nothing\" ACID transaction approach to prevent such corruption.\n",
       "\n",
       "\n",
       "## Snapshot Isolation\n",
       "\n",
       "In large environments with multiple concurrent readers and writers, metadata must be maintained so that the reads in progress act on consistent views of data and are not impacted by writes in progress. Delta Lake on Azure Databricks provides snapshot isolation, ensuring that multiple writers can write to a dataset simultaneously without interfering with jobs reading the dataset.\n",
       "\n",
       "\n",
       "## Schema Enforcement\n",
       "\n",
       "Notionally similar data but from different sources or of different vintage can differ in its representation, creating difficulties for using it effectively. Delta Lake helps ensure data integrity for ingested data by providing schema enforcement. Data can be stored using the preferred schema and potential data corruption with incorrect or invalid schemas is avoided.\n",
       ":selected: :selected:\n",
       "<!-- PageNumber=\"12\" -->\n",
       "\n",
       "<!-- PageHeader=\"Data Reliability\" -->\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/7)<!-- FigureContent=\"The image illustrates the processes of a \"Successful Write\" and a \"Failed Write\" in a database context:\n",
       "\n",
       "1. **Successful Write** (top section):\n",
       "   - A write request is initiated for a table in state \\(T_n\\).\n",
       "   - The table \\(T_n\\) is copied as \\(T_{n+1}\\) for the write, while \\(T_n\\) remains available.\n",
       "   - The write completes successfully.\n",
       "   - Finally, \\(T_n\\) is replaced with \\(T_{n+1}\\).\n",
       "\n",
       "2. **Failed Write** (bottom section):\n",
       "   - Similar to the successful write, a write request starts with the table in state \\(T_n\\).\n",
       "   - The table \\(T_n\\) is again copied as \\(T_{n+1}\\) for the write, with \\(T_n\\) still available.\n",
       "   - However, the write fails.\n",
       "   - As a result, \\(T_n\\) continues to be used.\n",
       "\n",
       "Both sections highlight the importance of maintaining availability of the original table during write operations, whether successful or failed.\" --></figure>\n",
       "\n",
       "\n",
       "\n",
       "## Exactly Once\n",
       "\n",
       "When working with long-running computations, multiple streams or concurrent batch jobs, there is a risk that some data will be missed (due to transmission difficulties) or duplicated (in attempts to correct for the misses). Delta Lake employs checkpointing to provide a robust exactly once delivery semantic that ensures that data is neither missed nor repeated erroneously.\n",
       "\n",
       "\n",
       "## UPSERTS and DELETES Support\n",
       "\n",
       "Standard Spark tables are write-once, i.e., they cannot be modified. Any necessary changes to account for late-arriving data or data that requires updating must be addressed using new tables. Delta Lake on Azure Databricks provides support for UPSERTS and DELETES. These commands provide a more \"convenient\" way of dealing with such changes.\n",
       ":unselected: :unselected: :unselected: :unselected: :unselected: :unselected:\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"13\" -->\n",
       " :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "System Complexity\n",
       "===\n",
       "\n",
       "System complexity is a key determinant not only of reliability and cost- effectiveness but also, very importantly, of responsiveness. As business requirements evolve, data analytics architecture needs to be flexible and responsive to keep up.\n",
       "\n",
       "\n",
       "## Unified Batch/Stream\n",
       "\n",
       "Delta Lake on Azure Databricks handles both batch and streaming data (via a direct integration with Structured Streaming for low latency updates). It can also concurrently write batch and streaming data to the same data table. Not only does this result in a simpler system architecture, but it also results in shorter time from data ingest to query result.\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/8)<!-- FigureContent=\"The image illustrates a data processing concept involving a \"Delta Table.\" \n",
       "\n",
       "- On the left, there are multiple input streams, represented by arrows pointing toward the delta table. \n",
       "- The central focus is the \"Delta Table,\" shown in a rectangular box and highlighted in blue. \n",
       "- To the right, there is a note indicating that the table is available for queries at all times.\n",
       "- Below the delta table, an arrow points upward labeled \"Batch updates,\" suggesting that updates to the table occur in batches.\n",
       "\n",
       "Overall, it represents a system where data streams feed into a table that can be queried continuously, with updates processed in batches.\" --></figure>\n",
       "\n",
       "\n",
       "\n",
       "### Schema Evolution\n",
       "\n",
       "Delta Lake on Azure Databricks can infer schema from input data. This reduces the effort for dealing with schema impact of changing business needs at multiple levels of the pipeline/data stack.\n",
       ":selected: :selected:\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"14\" -->\n",
       "\n",
       "Delta Lake on Azure Databricks Best Practices\n",
       "===\n",
       "\n",
       "Adopting the following best practices will help you make the most of Delta Lake on Azure Databricks.\n",
       "\n",
       "\n",
       "## Go Through Delta Lake\n",
       "\n",
       "All writes and reads should go through Delta Lake to ensure consistent overall behavior. Further, Delta Lake tables must not be accessed using earlier versions of the Azure Databricks Runtime because those versions do not understand Delta Lake and do not support it.\n",
       "\n",
       "\n",
       "## Run OPTIMIZE Regularly\n",
       "\n",
       "The OPTIMIZE command triggers compaction. It makes no data related changes to the table, so a read before and after an OPTIMIZE has the same results. It should be run regularly on tables that analysts are querying to ensure efficiency. A good starting point is to do this every day. Note that OPTIMIZE should not be run on base or staging tables.\n",
       "\n",
       "\n",
       "### Run VACUUM Regularly\n",
       "\n",
       "To ensure that concurrent readers can continue reading a stale snapshot of a table, Delta Lake on Azure Databricks leaves deleted files on DBFS for a period of time. The VACUUM command helps save on storage costs by cleaning up these invalid files. It can, however, interrupt users querying a Delta table similar to when partitions are re-written. VACUUM should be run regularly to clean up expired snapshots that are no longer required.\n",
       " :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<figure>\n",
       "\n",
       "![](figures/9)<!-- FigureContent=\"The image features a logo for \"Delta Lake.\" It includes a stylized triangle in a bright turquoise color positioned above a wavy blue shape, likely representing water. Beneath this graphic, the words \"DELTA LAKE\" are displayed in bold, dark gray letters, with \"DELTA\" and \"LAKE\" styled in a similar font emphasizing clarity and readability. The design suggests a connection to nature, particularly lakes or water bodies.\" --></figure>\n",
       "\n",
       " :selected: :selected:\n",
       "<!-- PageNumber=\"15\" -->\n",
       "\n",
       "<!-- PageHeader=\"Delta Lake on Azure Databricks Best Practices\" -->\n",
       "\n",
       "\n",
       "#### Batch Modifications\n",
       "\n",
       "The Parquet files that underpin Delta Lake are immutable and thus need to be rewritten completely to reflect changes regardless of the extent of the change. Use MERGE INTO to batch changes to amortize costs.\n",
       "\n",
       "\n",
       "## Use DELETES\n",
       "\n",
       "Manually deleting files from the underlying storage is likely to break the Delta Lake table. Instead, you should use DELETE commands to ensure proper progression of the change.\n",
       "\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"16\" -->\n",
       " :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "Trying Delta Lake on Azure Databricks\n",
       "===\n",
       " :selected: :selected: :selected: :selected:\n",
       "<figure>\n",
       "\n",
       "![](figures/10)<!-- FigureContent=\"The image features a logo for \"Delta Lake.\" It consists of a stylized triangle shape in a bright aqua color, which suggests a body of water or a delta. Beneath the triangle, the words \"DELTA LAKE\" are displayed in bold, dark gray letters. The overall design conveys a clean and modern aesthetic, likely representing a water-related theme or service.\" --></figure>\n",
       "\n",
       "\n",
       "Delta Lake is easy to put into production-Databricks on Azure users have been able to get into production with a Delta Lake-based solution in just a few weeks, using a small team compared to alternate approaches that take much longer and more resources. It is easy to get started with Delta Lake.\n",
       "\n",
       "Porting existing Spark code for using Delta Lake is as simple as changing \"CREATE TABLE USING parquet\" to \"CREATE TABLE . USING delta\"\n",
       "\n",
       "or changing\n",
       "\n",
       "\"dataframe.write. format(\"parquet\").load(\"/data/events\")\"\n",
       "\n",
       "\"dataframe.write. format(\"delta\"). load(\"/data/events\")\"\n",
       "\n",
       "If you are already using Azure Databricks, you can explore Delta Lake today using:\n",
       "\n",
       "· The Delta Lake on Azure Databricks quickstart\n",
       "\n",
       "· Optimization notebooks\n",
       "\n",
       "You can learn more about Delta Lake on Azure Databricks from the documentation.\n",
       ":selected: :selected:\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "<!-- PageNumber=\"17\" -->\n",
       " :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "Resources\n",
       "===\n",
       "\n",
       "\n",
       "## Documentation\n",
       "\n",
       ". Azure Databricks best practices\n",
       "\n",
       ". Introduction to Delta Lake\n",
       "\n",
       ". Introduction to MLFlow\n",
       "\n",
       "\n",
       "## Webinars\n",
       "\n",
       ". Get started with Apache Spark\n",
       "\n",
       ". Azure Databricks best practices\n",
       "\n",
       "· Machine Learning life cycle management with Azure Databricks and Azure Machine Learning\n",
       "\n",
       "\n",
       "## More info\n",
       "\n",
       "· Azure Databricks pricing page\n",
       "\n",
       "· Azure Databricks unit pre-purchase plan\n",
       "\n",
       "\n",
       "## Contact us\n",
       "\n",
       ". Talk to a sales expert\n",
       ":selected: :selected:\n",
       "Invent with purpose\n",
       "===\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/11)<!-- FigureContent=\"The image features the logo of Microsoft Azure. The background is a bright blue, and to the left, there are four colored squares arranged in a two-by-two grid. The top-left square is orange, the top-right square is green, the bottom-left square is blue, and the bottom-right square is yellow. Next to the squares, the text \"Microsoft Azure\" is prominently displayed in large, bold white letters.\" --></figure>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(docs[-1].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b63875-247a-4313-be9a-f33f5b9ff1fc",
   "metadata": {},
   "source": [
    "### Split the document into chunks base on markdown headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b0347d-42c0-41e4-96ae-ce0806bb4b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits: 26\n"
     ]
    }
   ],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\"),  \n",
    "    (\"#######\", \"Header 7\"), \n",
    "    (\"########\", \"Header 8\")\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_string = docs[0].page_content\n",
    "docs_result = text_splitter.split_text(docs_string)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(docs_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5704e50c-c7b5-44c7-865d-b9e4d658ebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Header 2': 'ACID Transactions'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_result[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc962d67-2cbb-4962-9d87-5a74ebd98e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:13:58.210953Z",
     "iopub.status.busy": "2024-07-19T13:13:58.209399Z",
     "iopub.status.idle": "2024-07-19T13:13:58.219798Z",
     "shell.execute_reply": "2024-07-19T13:13:58.216682Z",
     "shell.execute_reply.started": "2024-07-19T13:13:58.210953Z"
    }
   },
   "source": [
    "### Character Splitter to Split based on Chunk Size as well as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29494f5a-f48c-4ed2-8773-903507001cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits: 45\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "from langchain_text_splitters.base import Language, TextSplitter\n",
    "\n",
    "class CustomCharacterTextSplitter(TextSplitter):\n",
    "    \"\"\"Splitting text that looks at characters.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, separator: str = \"\\n\\n\", is_separator_regex: bool = False, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new TextSplitter.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self._separator = separator\n",
    "        self._is_separator_regex = is_separator_regex\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        # First we naively split the large input into a bunch of smaller ones.\n",
    "        separator = (\n",
    "            self._separator if self._is_separator_regex else re.escape(self._separator)\n",
    "        )\n",
    "        splits = re.split(separator, text, flags=re.DOTALL) \n",
    "        splits = [part for part in splits if part.strip()]\n",
    "        return splits\n",
    "\n",
    "text_splitter = CustomCharacterTextSplitter(separator=r'(<figure>.*?</figure>)', is_separator_regex=True)\n",
    "child_docs  = text_splitter.split_documents(docs_result)\n",
    "print(\"Length of splits: \" + str(len(child_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8c0da-fd73-4bd7-8ea2-fd47194d24ea",
   "metadata": {},
   "source": [
    "### Load the LangChain OpenAI Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bad5fdb5-7131-446c-bb87-83cc9190f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2024-03-01-preview\",\n",
    "    azure_endpoint =os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098fec1-de27-4941-9d5f-226abf50abb2",
   "metadata": {},
   "source": [
    "### Create the Azure AI Search Index Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05359122-9bc3-43fc-9d78-515f6a2de8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ScoringProfile,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    TextWeights\n",
    ")\n",
    "embedding_function = aoai_embeddings.embed_query\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        filterable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(embedding_function(\"Text\")),\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=False,\n",
    "    ),\n",
    "    # Additional field to store the title\n",
    "    SearchableField(\n",
    "        name=\"header\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    # Additional field for filtering on document source\n",
    "    SimpleField(\n",
    "        name=\"image\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        filterable=False,\n",
    "        searchable=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76eebb-a56d-4b2d-a729-f1cd3c7eb195",
   "metadata": {},
   "source": [
    "### Create the AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb7b3eff-2ece-48cd-b370-0dc750bf738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name: str = \"langchain-vector-demo-custom3\"\n",
    "\n",
    "vector_store_multi_modal: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=os.environ[\"AZURE_SEARCH_ENDPOINT\"],\n",
    "    azure_search_key=os.environ[\"AZURE_SEARCH_KEY\"],\n",
    "    index_name=index_name,\n",
    "    embedding_function=embedding_function,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3f8d686-f670-4378-8205-d6b8975c04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "def find_figure_indices(text):\n",
    "    pattern = r'!\\[\\]\\(figures/(\\d+)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    indices = [int(match) for match in matches]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10840850-da0e-4f35-b702-1b1c4dfd056a",
   "metadata": {},
   "source": [
    "### Ingest the chunks into Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c6974b2-304c-4ed4-9c81-6e7a3bf8829a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YjYwMzA2NTItZTM5NC00NTk1LWJkMWUtNDNkYmQyMDk1MTE5',\n",
       " 'MzBkNTUxMjQtYzViYy00YjllLWFhYzUtNGM2MjExNzljYWQ1',\n",
       " 'MGRiYjNmZDctY2JiZS00MjlkLThiOTEtMjg5MDE4ZWZlNzcz',\n",
       " 'MmM2YTlhMjUtZTIyMi00NDhiLTkyYjMtOTY2YWE2Njk1NjI0',\n",
       " 'ZWEyZGMwNmEtZDlhNi00ZGE4LWE5NmUtYjc4YjczZWY0NTE2',\n",
       " 'NGIzNTdjZTktODM3YS00OGVkLWIzNzgtNWE1MTQ4YTIyNTQ2',\n",
       " 'MjA1MDRhZmItZWEyNi00NDA4LTg0ZmUtNzM5OWY3MzM4Yjc0',\n",
       " 'YjFhYzMwYWQtMDgzMy00MjBlLTkzYmQtZmI1ZWQ0ZDliMDE1',\n",
       " 'NTg5MTVmODMtMjE2Zi00YmE5LWEyMDQtMDc5ZjkyM2RhZWZh',\n",
       " 'NTQ4NmJmZmItMzJjOC00OTZlLWJlNjMtNWQ0N2M2NGYwYWNj',\n",
       " 'OWZmMjRlM2MtZmQ4YS00ODNjLTgzNjUtYWRjOTE0NGE2MmRi',\n",
       " 'YTA0ZDcxN2EtZjFlZS00Y2QyLTg4YmUtM2M1MDVmOTEyOWQ2',\n",
       " 'OTlkN2Y2OGEtYTUzMi00OGFhLWI4N2YtNTI5OTIzZjI4NDI1',\n",
       " 'NWU1Y2IxNGItNmQwMi00NjY1LWFkMzQtMjQ5NGRjMGUyMGI5',\n",
       " 'NjUzMjc2NTYtODg1NC00YmI1LWJmOTgtY2MyMDU2MzUzNWEx',\n",
       " 'MTkyNDAwM2EtMTAwZi00M2NmLWEyMjMtMmI2YjI5ZWU4Nzdi',\n",
       " 'YzZmMTNhY2EtYmM1OC00YWRjLWFkMzAtNjA0YzcwZjg0OWY4',\n",
       " 'NTIxNzZmOWEtYjZlNy00NDM0LWE2NWYtZmFjYmE1YjQzM2My',\n",
       " 'MWE4NWY3YWQtNGZhMC00N2E0LTk5MjgtOTFjYWJhZmE4NDZk',\n",
       " 'ZTk0NmIwNzgtN2YwYy00MDFjLWI3MjQtYTYyYThiMTAwZDA4',\n",
       " 'MTU3NWJmODItMjM4Yy00ZmYwLTk3Y2QtMDIyOTc4MjIxNzg4',\n",
       " 'ZWIzNTIyMmItM2NmMi00MDk3LWE5NDUtMTAyODdkNTQ1N2Vj',\n",
       " 'NzE3MzM5MmMtNDQyNS00ODJjLTkyNmUtZDE5NWVhODk0Mjky',\n",
       " 'ZmY2ZWM4NDktOWY4OS00NGI4LWJhMjgtMDNlYTk2NWQzNWM1',\n",
       " 'MGY1MTFmY2ItMjM1My00MDUxLThiNTgtZDdjMDMxM2Q4ZmM4',\n",
       " 'ZTVjMmQ0MjgtOGU1ZS00Yzc5LWEyNWUtMjE5NDgzN2EyYzZh',\n",
       " 'YmU2MDQ2MDctMTE4MC00NmU4LTgwY2UtMWUzZTAwNWRhNDM2',\n",
       " 'YTU2MTE3ZWUtZWFhOC00OWIyLTg5N2MtYzRkNzU0YzM3OGQ0',\n",
       " 'MzVmMmEyNWYtODNiNy00YjJjLTljNDUtMzcyMTgxMWUzOTJl',\n",
       " 'NzA4MmZlYTItODgwZC00MDMwLTkwMzktYWI4NzI3MDBlZmQ4',\n",
       " 'NzhiNDEyNjctNTM0Yy00NGExLWIxMWYtMTQ0ODAyZDdkZTIz',\n",
       " 'ZjcxMmNkOWYtYjcyYi00MzlmLWFkNWEtMzRjMTdkY2I4YTM2',\n",
       " 'M2Q2YzM1M2MtMDA5My00ZDI5LTk3OGEtNzBlMDE2MzkxNDAy',\n",
       " 'ZWEwYjkxZmEtZDMyYi00MDI2LTkzZmMtOTUyYTM3NDhlM2Zm',\n",
       " 'NDhkYWViYTYtNmZhZC00ZTkyLWJhZjYtMjI4ZWExMDRkNTAx',\n",
       " 'ZjVmZjliY2QtZGIxYi00YjU0LTlkNGMtMmY1ZjZjYTlhY2I3',\n",
       " 'MjQ0MTVlMTktODJkMi00MjQyLThlMzMtYzdmMTc2Y2Q1M2Zj',\n",
       " 'ZWE1M2FkNjUtZjg5Yy00OGI0LWI1M2YtODUwNmM4ZjUyOTk2',\n",
       " 'MjhlZWEyYmUtZDRhNS00ZWE0LWI5YTMtMGQ5MjBmYzA2ZTQ2',\n",
       " 'NDRmNDkzNWItNWQyNy00YmQ1LThjMWYtNGU1YTA5MmUzNDgx',\n",
       " 'NzM4MjczNDAtMGJhYi00ZmRiLWI0M2QtOWNjMmU4YTkxYmU1',\n",
       " 'YzBkNWZkZDctMDMyZS00ZmIzLTk4MTktZWE0NzY0MWUyMTcw',\n",
       " 'MjVkZWRlYjItNThmYS00NDAzLWIxZjAtOTA2ZDVjYTQxMGM0',\n",
       " 'NjNlZTRhOTMtOTEwMi00ZjdjLWI1NzItODY0NTdkMTRkMjM3',\n",
       " 'ZDI0MDI3NzMtZTI3OS00ZTQ5LWI1NmQtYjRiMTNiMTJiZjJh']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_metadata = docs[-1].metadata['images']\n",
    "lst_docs = []\n",
    "for doc in child_docs:\n",
    "    figure_indices = find_figure_indices(doc.page_content)\n",
    "    if figure_indices:\n",
    "        for figure_indice in figure_indices:\n",
    "            image = image_metadata[figure_indice]\n",
    "            doc_result = Document(page_content = doc.page_content, metadata={\"header\": json.dumps(doc.metadata), \"source\": \"sam.pdf\", \"image\": image})\n",
    "            lst_docs.append(doc_result)\n",
    "    else:\n",
    "        doc_result = Document(page_content = doc.page_content, metadata={\"header\": json.dumps(doc.metadata), \"source\": \"sam.pdf\", \"image\": None})\n",
    "        lst_docs.append(doc_result)\n",
    "vector_store_multi_modal.add_documents(documents=lst_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90bff248-b474-4a6d-9453-44bfd32236e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = await FAISS.afrom_documents(documents=child_docs, embedding=aoai_embeddings)\n",
    "retriever_base = index.as_retriever(search_type=\"similarity\",search_kwargs = {\"k\" : 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31d4ed9e-ee4f-40fe-850b-fad0b903200d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'AzureOpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024A48D0C290>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8643ba3-ae51-4357-a2e0-cfc1a46e813f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lets do the RAG Now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6216c-d26f-4845-8e8a-ccf1d34fb0ae",
   "metadata": {},
   "source": [
    "### Load the AOAI Chat Class from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "143f0c4a-0fe7-4d2c-8b8b-f1274f5ceba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_878413d04d'}, id='run-93384609-b229-4675-959d-2910e615a3bb-0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(api_key = os.environ[\"AZURE_OPENAI_API_KEY\"],  \n",
    "                      api_version = \"2024-06-01\",\n",
    "                      azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "                      model= os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "                      streaming=True)\n",
    "llm([HumanMessage(\"Hi\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73033ec9-2458-4002-bb2b-c080cb7cc703",
   "metadata": {},
   "source": [
    "### Multi Modal RAG (Ingestion Side Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35e4c2f9-0727-49bd-95af-c13150223e7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachinichake\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\hub.py:80: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  resp: str = client.pull(owner_repo_commit)\n",
      "C:\\Users\\sachinichake\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchainhub\\client.py:326: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = self.pull_repo(owner_repo_commit)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 19\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m to_return\n\u001b[0;32m      9\u001b[0m rag_chain_from_docs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     10\u001b[0m     {\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28minput\u001b[39m: format_docs(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m rag_chain_with_source \u001b[38;5;241m=\u001b[39m \u001b[43mRunnableMap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m \u001b[38;5;241m|\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28minput\u001b[39m: [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: rag_chain_from_docs,\n\u001b[0;32m     24\u001b[0m }\n\u001b[0;32m     25\u001b[0m rag_chain_with_source\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many papers was released in the year 2021 with LLMs + Alignment?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36mRunnableParallel.__init__\u001b[1;34m(self, _RunnableParallel__steps, **kwargs)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__steps} \u001b[38;5;28;01mif\u001b[39;00m __steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[38;5;241m=\u001b[39m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmerged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__steps} \u001b[38;5;28;01mif\u001b[39;00m __steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[38;5;241m=\u001b[39m{key: \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, r \u001b[38;5;129;01min\u001b[39;00m merged\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\schema\\runnable\\base.py:2527\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   2525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "def format_docs(docs):\n",
    "    to_return =  \"\\n\\n\".join(str(doc.metadata) + \"\\n\" + doc.page_content for doc in docs)\n",
    "    return to_return\n",
    "    \n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableMap(\n",
    "    {\"documents\": retriever_base, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "rag_chain_with_source.invoke(\"How many papers was released in the year 2021 with LLMs + Alignment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61eea339-2b2f-4332-add7-cdf739f02f4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_chain_with_source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrag_chain_with_source\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich module is present between pre training and reward modelling?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rag_chain_with_source' is not defined"
     ]
    }
   ],
   "source": [
    "rag_chain_with_source.invoke(\"Which module is present between pre training and reward modelling?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "882efb51-8166-4e96-a998-1ec4db0d3aab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:38:12.565033Z",
     "iopub.status.busy": "2024-07-19T13:38:12.564502Z",
     "iopub.status.idle": "2024-07-19T13:38:15.783874Z",
     "shell.execute_reply": "2024-07-19T13:38:15.782178Z",
     "shell.execute_reply.started": "2024-07-19T13:38:12.565033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'Header 1': '2\\\\. Background',\n",
       "   'Header 2': '2.10. Pre-Training Objectives'},\n",
       "  {'Header 1': '3\\\\. Large Language Models'},\n",
       "  {'Header 1': '2\\\\. Background', 'Header 2': '2.5. Layer Normalization'},\n",
       "  {'Header 1': '1\\\\. Introduction'},\n",
       "  {'Header 1': '2\\\\. Background'}],\n",
       " 'answer': 'The components of RLHF shown in green dashed lines include the **LLM** and **Reward**.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_with_source.invoke(\"Which component are part of RLHF shown in green dash lines?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4434cf-daa1-4920-8e1f-c038b29dfb17",
   "metadata": {},
   "source": [
    "### Multi Modal RAG (Both Ingestion Side + Calling Side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "793d1810-1bd7-4cbc-bc92-b623138f98d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'langchain.schema.output_parser.StrOutputParser'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 45\u001b[0m\n\u001b[0;32m     41\u001b[0m retriever_multi_modal \u001b[38;5;241m=\u001b[39m vector_store_multi_modal\u001b[38;5;241m.\u001b[39mas_retriever(search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# RAG pipeline\u001b[39;00m\n\u001b[0;32m     44\u001b[0m chain_multimodal_rag \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 45\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever_multi_modal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRunnableLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_image_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRunnablePassthrough\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRunnableLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_prompt_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mStrOutputParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# chain_multimodal_rag.invoke(\"Which component are part of RLHF shown in green dash lines?\") \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2818\u001b[0m, in \u001b[0;36mRunnableSequence.__or__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   2804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\n\u001b[0;32m   2805\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst,\n\u001b[0;32m   2806\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2811\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mor\u001b[39;00m other\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   2812\u001b[0m     )\n\u001b[0;32m   2813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\n\u001b[0;32m   2815\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst,\n\u001b[0;32m   2816\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle,\n\u001b[0;32m   2817\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast,\n\u001b[1;32m-> 2818\u001b[0m         \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   2819\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   2820\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5553\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   5551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   5552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   5554\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5555\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5556\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'langchain.schema.output_parser.StrOutputParser'>"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def get_image_text(docs):\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        if doc.metadata['image']:\n",
    "            b64_images.append(doc.metadata['image'])\n",
    "        else:\n",
    "            texts.append(doc.page_content)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "retriever_multi_modal = vector_store_multi_modal.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain_multimodal_rag = (\n",
    "    {\n",
    "        \"context\": retriever_multi_modal | RunnableLambda(get_image_text),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(img_prompt_func)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_multimodal_rag.invoke(\"Which component are part of RLHF shown in green dash lines?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2003f7-4bad-452e-a490-af56d47af7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
